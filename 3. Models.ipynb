{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import unicodedata\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(df_train_x.shape)\\nprint(type(df_train_x))\\nprint(df_train_y.shape)\\nprint(type(df_train_y))\\nprint(df_test_x.shape)\\nprint(type(df_test_x))\\nprint(df_test_y.shape)\\nprint(type(df_test_y))\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_localpolice_union_all = pd.read_csv('accidents_localpolice_homogenized_2010to2020.csv', delimiter=',',\n",
    "                                       encoding='utf8')\n",
    "df_localpolice_union_all = df_localpolice_union_all[df_localpolice_union_all['nom_districte'] != 'Desconegut'].copy()\n",
    "df_localpolice_union_all.drop(\n",
    "    columns=['hora_dia', 'codi_barri', 'codi_carrer', 'codi_districte', 'coordenada_utm_x', 'coordenada_utm_y',\n",
    "             'descripcio_causa_vianant', 'nom_barri', 'nom_carrer', 'numero_expedient', 'Full_Date'], inplace=True)\n",
    "df_localpolice_union_all.drop(\n",
    "    columns=['dia_setmana', 'numero_lesionats_greus', 'numero_lesionats_lleus', 'numero_vehicles_implicats'],\n",
    "    inplace=True)\n",
    "\n",
    "# Group by to make predictions by district and month\n",
    "# the previous aggregation level was too low, which made impossible to predict all random variations\n",
    "# grouping by on a monthly basis randomness gets diluted and compensated\n",
    "df_localpolice_union_all = df_localpolice_union_all.groupby(['nom_districte', 'any', 'mes_any']).agg(\n",
    "    {'numero_victimes': 'sum', 'COVID': 'mean'}).reset_index()\n",
    "\n",
    "# Lines to drop Covid feature or include it\n",
    "# Since we do not have enough historic data with covid, the ML cannot be trained properly\n",
    "# df_localpolice_union_all.drop(columns=['COVID'], inplace=True)\n",
    "# df_localpolice_union_all = df_localpolice_union_all.groupby(['nom_districte', 'any', 'mes_any']).agg({'numero_victimes': 'sum'}).reset_index()\n",
    "\n",
    "# One hot encoding: models need numbers without overvalue high numbers 1to12 months would score higher decembers otherwise\n",
    "one_hot_encoding = pd.get_dummies(df_localpolice_union_all.mes_any, prefix='mes_any')\n",
    "df_localpolice_union_all = pd.concat([df_localpolice_union_all, one_hot_encoding], axis=1)\n",
    "one_hot_encoding = pd.get_dummies(df_localpolice_union_all.nom_districte, prefix='nom_districte')\n",
    "df_localpolice_union_all = pd.concat([df_localpolice_union_all, one_hot_encoding], axis=1)\n",
    "df_localpolice_union_all.drop(columns=['mes_any', 'nom_districte'], inplace=True)\n",
    "\n",
    "# Decide which Library and target value you want to predict TPOT or MLJAR\n",
    "type_of_model = 'TPOT'\n",
    "variable_objetivo = 'numero_victimes'\n",
    "\n",
    "# Split train dataset, take at least some months with covid to provide the train samples some historic data about covid\n",
    "df_train_x = df_localpolice_union_all[df_localpolice_union_all['any'] < 2020].copy()\n",
    "df_train_x = df_train_x.append(df_localpolice_union_all.query(\n",
    "    'any == 2020 & mes_any_9 != 1 & mes_any_10 != 1 & mes_any_11 != 1 & mes_any_12 != 1'), ignore_index=True)\n",
    "\n",
    "df_train_x = df_train_x.fillna(0)\n",
    "df_train_y = df_train_x[variable_objetivo]\n",
    "df_train_x.drop(columns=[variable_objetivo], inplace=True)\n",
    "\n",
    "# We would also need some months of Covid\n",
    "df_test_x = df_localpolice_union_all.query(\n",
    "    '(any == 2020 & mes_any_9 == 1) | (any == 2020 & mes_any_10 == 1) | (any == 2020 & mes_any_11 == 1) | (any == 2020 & mes_any_12 == 1)')\n",
    "\n",
    "df_test_x = df_test_x.fillna(0)\n",
    "df_test_y = df_test_x[variable_objetivo]\n",
    "df_test_x.drop(columns=[variable_objetivo], inplace=True)\n",
    "\n",
    "# Make sure they are numpy arrays\n",
    "df_train_x = df_train_x.to_numpy()\n",
    "df_train_y = df_train_y.to_numpy()\n",
    "df_test_x = df_test_x.to_numpy()\n",
    "df_test_y = df_test_y.to_numpy()\n",
    "\n",
    "df_train_x = np.where(np.isnan(df_train_x), 0, df_train_x)\n",
    "df_train_y = np.where(np.isnan(df_train_y), 0, df_train_y)\n",
    "df_test_x = np.where(np.isnan(df_test_x), 0, df_test_x)\n",
    "df_test_y = np.where(np.isnan(df_test_y), 0, df_test_y)\n",
    "\n",
    "# Verify its shape and type\n",
    "\"\"\"\n",
    "print(df_train_x.shape)\n",
    "print(type(df_train_x))\n",
    "print(df_train_y.shape)\n",
    "print(type(df_train_y))\n",
    "print(df_test_x.shape)\n",
    "print(type(df_test_x))\n",
    "print(df_test_y.shape)\n",
    "print(type(df_test_y))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\anaconda3\\lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6979f226da69454a9ab715bbb86f6040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=1300.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.25478163139274895\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.3873950584073153\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.3873950584073153\n",
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: AdaBoostRegressor(LinearSVR(Binarizer(input_matrix, threshold=0.1), C=10.0, dual=True, epsilon=1.0, loss=epsilon_insensitive, tol=0.01), learning_rate=0.01, loss=linear, n_estimators=100)\n",
      "14/06/2021-22:25:48 INFO: model score: \n",
      "14/06/2021-22:25:48 INFO: predictions: [ 47.  47.  47.  47. 118. 232. 127. 118.  47.  47.  47.  47.  47.  47.\n",
      "  47.  47.  47.  47.  47.  47.  47.  47.  47.  47.  47.  47.  47.  47.\n",
      "  68.  68.  68.  68.  67.  68.  68.  68.  68.  68.  68.  68.]\n",
      "14/06/2021-22:25:48 INFO: real: [ 47  23  29  17 185 202 156 208  35  26  19  35  56  37  54  39  55  55\n",
      "  43  48  51  38  45  50  55  61  36  50  73  75  81  77  89  72  61  90\n",
      "  72  75 100  91]\n",
      "0\n",
      "14/06/2021-22:25:48 INFO: mape detail: [1.         0.         0.37931034 0.         0.63783784 0.85148515\n",
      " 0.81410256 0.56730769 0.65714286 0.19230769 0.         0.65714286\n",
      " 0.83928571 0.72972973 0.87037037 0.79487179 0.85454545 0.85454545\n",
      " 0.90697674 0.97916667 0.92156863 0.76315789 0.95555556 0.94\n",
      " 0.85454545 0.7704918  0.69444444 0.94       0.93150685 0.90666667\n",
      " 0.83950617 0.88311688 0.75280899 0.94444444 0.8852459  0.75555556\n",
      " 0.94444444 0.90666667 0.68       0.74725275]\n",
      "14/06/2021-22:25:48 INFO: mape: 0.7400777006025626\n",
      "14/06/2021-22:25:48 INFO: mape weighted: 0.7779417189229063\n",
      "14/06/2021-22:25:48 INFO: r2: 0.7369910011197129\n",
      "14/06/2021-22:25:48 INFO: mae: 15.625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# functions to calculate accuracies\n",
    "def f_calculate_mape(y_true, y_pred):\n",
    "    y_pred = np.where(y_pred > 0, y_pred, 0)\n",
    "    # record to record calculations\n",
    "    fa = np.zeros(len(y_true))\n",
    "    for i in range(len(y_true)):\n",
    "        bias = abs(y_true[i] - y_pred[i])\n",
    "        if (y_true[i] == 0) & (y_pred[i] == 0):\n",
    "            er = 0\n",
    "        elif y_pred[i] == 0:\n",
    "            er = bias / (1 + abs(y_true[i]))\n",
    "        else:\n",
    "            er = bias / y_true[i]\n",
    "        fa[i] = 1 - er\n",
    "    # Do not take into account forecasts accuracies equal to 0\n",
    "    fa = fa[~np.isnan(fa)]\n",
    "    fa[fa < 0] = 0\n",
    "    return fa\n",
    "\n",
    "\n",
    "def f_calculate_mape_weighted(y_true, y_pred):\n",
    "    fa = f_calculate_mape(y_true, y_pred)\n",
    "    sum_total = sum(y_true)\n",
    "    if sum_total != 0:\n",
    "        perc_liters_sold = y_true / sum_total\n",
    "    else:\n",
    "        perc_liters_sold = 0\n",
    "    if sum(y_true == y_pred) == len(y_true):\n",
    "        res = 1\n",
    "    else:\n",
    "        res = sum(fa * perc_liters_sold)\n",
    "    return res\n",
    "\n",
    "\n",
    "if type_of_model == 'TPOT':\n",
    "\n",
    "    #############################################\n",
    "    # AUTO ML - TPOT\n",
    "    #############################################\n",
    "\n",
    "    # conda install -c conda-forge tpot\n",
    "    from tpot import TPOTRegressor\n",
    "    from sklearn.datasets import load_boston\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Un-comment here to train the model\n",
    "    tpot = TPOTRegressor(generations=25, population_size=50, verbosity=2, random_state=0, n_jobs=-1, scoring='r2')\n",
    "    # Train\n",
    "    tpot.fit(df_train_x, df_train_y)\n",
    "    # Tpot generates a file that need to be cleaned in order to use the selected model\n",
    "    model = tpot\n",
    "\n",
    "    tpot.export('tpot_pipeline.py')\n",
    "    # to use previous generated models by tpot and do not run tpot each time, we can clean the exported files\n",
    "    # we only get the header, imports and parameters with the best select model by tpot the we just fit that model\n",
    "    \"\"\"\n",
    "    with open('tpot_pipeline.py', \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    with open('tpot_pipeline.py', \"w\") as f:\n",
    "        line_to_keep = False\n",
    "        for line in lines:\n",
    "            if line_to_keep is True:\n",
    "                if line.strip(\"\\n\") != \")\":\n",
    "                    f.write(line)\n",
    "                else:\n",
    "                    f.write(line)\n",
    "                    break\n",
    "                    # If there are no further lines, exit\n",
    "            elif (\"import\" in line) | (\"from\" in line):\n",
    "                f.write(line)\n",
    "            elif \"exported_pipeline\" in line.strip(\"\\n\"):\n",
    "                line_to_keep = True\n",
    "                f.write(line)\n",
    "                if \")\" == line.strip(\"\\n\")[-1:]:\n",
    "                    break\n",
    "    \n",
    "    from tpot_pipeline import exported_pipeline\n",
    "    model = exported_pipeline.fit(df_train_x, df_train_y)\n",
    "    \"\"\"\n",
    "\n",
    "    # scores\n",
    "    print(\"{0} INFO: model score: \".format(datetime.datetime.now().strftime('%d/%m/%Y-%H:%M:%S'),\n",
    "                                           model.score(df_test_x, df_test_y)))\n",
    "    # print(model.score(df_test_x, df_test_y))\n",
    "    predictions = model.predict(df_test_x)\n",
    "    predictions[predictions < 0] = 0\n",
    "    predictions = np.rint(predictions)\n",
    "    print(\"{0} INFO: predictions: {1}\".format(datetime.datetime.now().strftime('%d/%m/%Y-%H:%M:%S'), predictions))\n",
    "    # print(predictions)\n",
    "    print(\"{0} INFO: real: {1}\".format(datetime.datetime.now().strftime('%d/%m/%Y-%H:%M:%S'), df_test_y))\n",
    "    # print(df_test_y)\n",
    "    print(0)\n",
    "\n",
    "elif type_of_model == 'MLJAR':\n",
    "    #############################################\n",
    "    # AUTO ML - MLJAR https://supervised.mljar.com/api/\n",
    "    #############################################\n",
    "    from supervised.automl import AutoML\n",
    "\n",
    "    # configure AutoML based on its official documentation\n",
    "    automl = AutoML(eval_metric='r2', n_jobs=-1, golden_features=True, stack_models=True, train_ensemble=True,\n",
    "                    validation_strategy={\"validation_type\": \"kfold\", \"k_folds\": 5, \"shuffle\": True, \"stratify\": True,\n",
    "                                         \"random_seed\": 123})\n",
    "    # train models with AutoML\n",
    "    automl.fit(df_train_x, df_train_y)\n",
    "\n",
    "    # compute the MSE on test data\n",
    "    predictions = automl.predict(df_test_x)\n",
    "    print(0)\n",
    "\n",
    "mape = f_calculate_mape(df_test_y, predictions)\n",
    "print(\"{0} INFO: mape detail: {1}\".format(datetime.datetime.now().strftime('%d/%m/%Y-%H:%M:%S'), mape))\n",
    "print(\"{0} INFO: mape: {1}\".format(datetime.datetime.now().strftime('%d/%m/%Y-%H:%M:%S'), np.average(mape)))\n",
    "mape_weighted = f_calculate_mape_weighted(df_test_y, predictions)\n",
    "print(\"{0} INFO: mape weighted: {1}\".format(datetime.datetime.now().strftime('%d/%m/%Y-%H:%M:%S'), mape_weighted))\n",
    "r2 = r2_score(df_test_y, predictions)\n",
    "print(\"{0} INFO: r2: {1}\".format(datetime.datetime.now().strftime('%d/%m/%Y-%H:%M:%S'), r2))\n",
    "mae = mean_absolute_error(df_test_y, predictions)\n",
    "print(\"{0} INFO: mae: {1}\".format(datetime.datetime.now().strftime('%d/%m/%Y-%H:%M:%S'), mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
